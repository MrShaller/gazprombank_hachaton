import sys
import types
import pandas as pd
from backend.app.ml.pipeline import InferencePipeline
from backend.app.ml.utils import tokenize_lemma

# --- —Ñ–∏–∫—Å –¥–ª—è joblib (–æ–∂–∏–¥–∞–µ—Ç tokenize_lemma –≤ —Å—Ç–∞—Ä–æ–º –º–µ—Å—Ç–µ) ---
fake_module = types.ModuleType("scripts.models.inference_pipeline")
fake_module.tokenize_lemma = tokenize_lemma
sys.modules["scripts.models.inference_pipeline"] = fake_module

OUT_FINAL = "data/processed/result_final.json"

# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
pipe = InferencePipeline(
    tfidf_path="models/tfidf_lr/model.pkl",
    xlmr_path="models/xlmr"
)

# üîπ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ–±—ã—á–Ω–æ –ø—Ä–∏—Ö–æ–¥—è—Ç –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –≤ FastAPI)
sample_json = [
    {"id": 1, "text": "–ë–∞–Ω–∫ –æ–±–º–∞–Ω—É–ª –º–µ–Ω—è —Å –Ω–∞—á–∏—Å–ª–µ–Ω–∏–µ–º –∫–µ—à–±—ç–∫–∞!"},
    {"id": 2, "text": "–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ –∫–∞—Ä—Ç–∞ —É–∂–∞—Å–Ω–∞—è."}
]
# üîπ —á–∏—Ç–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π JSON
#with open("data/raw/test_request2.json", "r", encoding="utf-8") as f:
#    raw = json.load(f)

# –¥–æ—Å—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫
#sample_json = raw["data"]
# 1Ô∏è‚É£ –ü–æ–∫–ª–∞—É–∑–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑ JSON
df_clauses = pipe.run_from_json(sample_json)
print("=== –ü–æ–∫–ª–∞—É–∑–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ===")
print(df_clauses.head())

# 2Ô∏è‚É£ –§–∏–Ω–∞–ª—å–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è –∏–∑ JSON
df_final = pipe.run_and_aggregate_from_json(sample_json)
print("=== –§–∏–Ω–∞–ª—å–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è ===")
print(df_final.head())

# –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ JSON
df_final.to_json(OUT_FINAL, orient="records", force_ascii=False, indent=2)
print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {OUT_FINAL}")