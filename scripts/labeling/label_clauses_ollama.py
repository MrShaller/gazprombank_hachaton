#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–†–∞–∑–º–µ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–ª–∞—É–∑ –ø–æ –±–∞–Ω–∫–æ–≤—Å–∫–∏–º –ø—Ä–æ–¥—É–∫—Ç–∞–º –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é Ollama

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–ª–∞—É–∑—ã –∏–∑ CSV —Ñ–∞–π–ª–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM (Ollama) 
–¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ –∏—Ö —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.
"""

import pandas as pd
import json
import subprocess
import time
import os
from pathlib import Path
from typing import List, Dict, Any, Tuple
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OllamaClauseLabeler:
    def __init__(self, model_name: str = "gpt-oss:20b"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞ –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ –∫–ª–∞—É–∑
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
        """
        self.model_name = model_name
        self.allowed_topics = [
            "–î–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞", "–ö—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞", "–î–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ", 
            "–î—Ä—É–≥–æ–µ", "–î–µ–Ω–µ–∂–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã", "–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–π –∫—Ä–µ–¥–∏—Ç", "–ò–ø–æ—Ç–µ–∫–∞", 
            "–ê–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç", "–†–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–æ–≤", "–í–∫–ª–∞–¥—ã", 
            "–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ", "–û–±–º–µ–Ω –≤–∞–ª—é—Ç"
        ]
        self.allowed_sentiments = ["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ", "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ", "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ"]
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤
        self.review_texts = {}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
        self._check_ollama_availability()
    
    def _check_ollama_availability(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama –∏ –º–æ–¥–µ–ª–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
            result = subprocess.run(['ollama', 'list'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode != 0:
                raise RuntimeError("Ollama –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
            if self.model_name not in result.stdout:
                logger.warning(f"–ú–æ–¥–µ–ª—å {self.model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏...")
                self._pull_model()
            
            logger.info(f"Ollama –∏ –º–æ–¥–µ–ª—å {self.model_name} –≥–æ—Ç–æ–≤—ã –∫ —Ä–∞–±–æ—Ç–µ")
            
        except subprocess.TimeoutExpired:
            raise RuntimeError("Timeout –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ Ollama")
        except FileNotFoundError:
            raise RuntimeError("Ollama –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Å https://ollama.ai/")
    
    def _pull_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –æ–Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {self.model_name}...")
            result = subprocess.run(['ollama', 'pull', self.model_name], 
                                  capture_output=True, text=True, timeout=300)
            if result.returncode != 0:
                raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {result.stderr}")
            logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except subprocess.TimeoutExpired:
            raise RuntimeError("Timeout –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏")
    
    def load_review_texts(self, merged_json_path: str):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ merged.json
        
        Args:
            merged_json_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É merged.json
        """
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ {merged_json_path}")
        
        try:
            with open(merged_json_path, 'r', encoding='utf-8') as f:
                reviews_data = json.load(f)
            
            for review in reviews_data:
                review_id = review['review_id']
                review_text = review['review_text']
                self.review_texts[review_id] = review_text
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.review_texts)} –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤: {e}")
            raise
    
    def create_prompt(self, clause: str, full_review_text: str = None) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM
        
        Args:
            clause: –¢–µ–∫—Å—Ç –∫–ª–∞—É–∑—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            full_review_text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –ì–æ—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ LLM
        """
        context_section = ""
        if full_review_text:
            context_section = f"""
–ö–û–ù–¢–ï–ö–°–¢ - –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞:
"{full_review_text}"

"""

        prompt = f"""–¢—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –∫–ª–∞—É–∑. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äì –≤—ã–¥–µ–ª—è—Ç—å –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã/—É—Å–ª—É–≥–∏ –∏ –∏—Ö —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–ª–∞—É–∑–µ, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—Å–µ–≥–æ –æ—Ç–∑—ã–≤–∞.

{context_section}–ü—Ä–∞–≤–∏–ª–∞:
- –†–∞–∑–º–µ—á–∞–π —Ç–æ–ª—å–∫–æ –ø–æ —Å–ø–∏—Å–∫—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {self.allowed_topics}.
- –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–ª—å–∫–æ –∏–∑ —Å–ø–∏—Å–∫–∞: {self.allowed_sentiments}.
- –í–ê–ñ–ù–û: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –°–¢–†–û–ì–û –†–ê–í–ù–´–ú. –ù–∞ –∫–∞–∂–¥—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç/—É—Å–ª—É–≥—É –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å.
- –ï—Å–ª–∏ –≤ –∫–ª–∞—É–∑–µ –Ω–µ—Ç –Ø–í–ù–´–• –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤, –æ—Å—Ç–∞–≤—å –æ–±–∞ —Å–ø–∏—Å–∫–∞ –ø—É—Å—Ç—ã–º–∏.
- –ï—Å–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ—è—Å–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–π "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ".
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–º–µ–Ω–Ω–æ –¥–∞–Ω–Ω—É—é –∫–ª–∞—É–∑—É, –Ω–æ —É—á–∏—Ç—ã–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—Å–µ–≥–æ –æ—Ç–∑—ã–≤–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.
- –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.

–ü—Ä–∏–º–µ—Ä—ã:
–í—Ö–æ–¥: "–û—á–µ–Ω—å –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏, –Ω–æ –º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —á–∞—Å—Ç–æ –∑–∞–≤–∏—Å–∞–µ—Ç."
–í—ã—Ö–æ–¥:
{{
  "topics": ["–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ", "–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"],
  "sentiments": ["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ", "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ"]
}}

–í—Ö–æ–¥: "–í–∑—è–ª –∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç."
–í—ã—Ö–æ–¥:
{{
  "topics": ["–ê–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç"],
  "sentiments": ["–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ"]
}}

–í—Ö–æ–¥: "–ü—Ä–∏—à–µ–ª –≤ –±–∞–Ω–∫ —É—Ç—Ä–æ–º."
–í—ã—Ö–æ–¥:
{{
  "topics": [],
  "sentiments": []
}}

–¢–µ–ø–µ—Ä—å —Ä–∞–∑–º–µ—á–∞–π —Å–ª–µ–¥—É—é—â—É—é –∫–ª–∞—É–∑—É:
"{clause}"
"""
        return prompt
    
    def call_ollama(self, prompt: str, max_retries: int = 3) -> Dict[str, Any]:
        """
        –í—ã–∑–æ–≤ Ollama –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–º–ø—Ç–∞
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
            
        Returns:
            –û—Ç–≤–µ—Ç LLM –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è
        """
        for attempt in range(max_retries):
            try:
                # –í—ã–∑—ã–≤–∞–µ–º Ollama —á–µ—Ä–µ–∑ subprocess
                result = subprocess.run(
                    ['ollama', 'run', self.model_name],
                    input=prompt,
                    capture_output=True,
                    text=True,
                    timeout=60  # –¢–∞–π–º–∞—É—Ç 60 —Å–µ–∫—É–Ω–¥
                )
                
                if result.returncode != 0:
                    logger.warning(f"Ollama –≤–µ—Ä–Ω—É–ª–∞ –æ—à–∏–±–∫—É (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {result.stderr}")
                    continue
                
                # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
                response_text = result.stdout.strip()
                
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
                try:
                    # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–µ—Ä–Ω—É—Ç –≤ –¥—Ä—É–≥–æ–π —Ç–µ–∫—Å—Ç)
                    start_idx = response_text.find('{')
                    end_idx = response_text.rfind('}') + 1
                    
                    if start_idx != -1 and end_idx > start_idx:
                        json_str = response_text[start_idx:end_idx]
                        response_data = json.loads(json_str)
                        
                        # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
                        if self._validate_response(response_data):
                            return response_data
                        else:
                            logger.warning(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {response_data}")
                    else:
                        logger.warning(f"JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {response_text}")
                        
                except json.JSONDecodeError as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                    logger.warning(f"–û—Ç–≤–µ—Ç: {response_text}")
                
            except subprocess.TimeoutExpired:
                logger.warning(f"Timeout –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Ollama (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Ollama (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        logger.error("–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –≤—ã–∑–æ–≤–∞ Ollama –Ω–µ—É–¥–∞—á–Ω—ã")
        return {"topics": [], "sentiments": []}
    
    def _validate_response(self, response: Dict[str, Any]) -> bool:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM
        
        Args:
            response: –û—Ç–≤–µ—Ç –æ—Ç LLM
            
        Returns:
            True –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –≤–∞–ª–∏–¥–Ω—ã–π
        """
        if not isinstance(response, dict):
            return False
        
        if "topics" not in response or "sentiments" not in response:
            return False
        
        if not isinstance(response["topics"], list) or not isinstance(response["sentiments"], list):
            return False
        
        # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π
        if len(response["topics"]) != len(response["sentiments"]):
            logger.warning(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ ({len(response['topics'])}) –Ω–µ —Ä–∞–≤–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π ({len(response['sentiments'])})")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ç–æ–ø–∏–∫–∏ –∏–∑ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
        for topic in response["topics"]:
            if topic not in self.allowed_topics:
                logger.warning(f"–ù–µ—Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–π —Ç–æ–ø–∏–∫: {topic}")
                return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
        for sentiment in response["sentiments"]:
            if sentiment not in self.allowed_sentiments:
                logger.warning(f"–ù–µ—Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}")
                return False
        
        return True
    
    def process_clause(self, clause_row: pd.Series) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∫–ª–∞—É–∑—ã
        
        Args:
            clause_row: –°—Ç—Ä–æ–∫–∞ DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—É–∑—ã
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–∑–º–µ—Ç–∫–∏ –∫–ª–∞—É–∑—ã
        """
        clause_text = clause_row['clause']
        clause_id = clause_row['clause_id']
        review_id = clause_row['review_id']
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        full_review_text = self.review_texts.get(review_id, None)
        if full_review_text is None:
            logger.warning(f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ {review_id} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∫–ª–∞—É–∑—É")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        prompt = self.create_prompt(clause_text, full_review_text)
        
        # –í—ã–∑—ã–≤–∞–µ–º LLM
        llm_response = self.call_ollama(prompt)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "clause_id": clause_id,
            "review_id": review_id,
            "clause": clause_text,
            "topics": llm_response.get("topics", []),
            "sentiments": llm_response.get("sentiments", []),
            "has_full_context": full_review_text is not None
        }
        
        return result
    
    def process_clauses_batch(self, clauses_df: pd.DataFrame, 
                            start_idx: int = 0, batch_size: int = 1000) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –∫–ª–∞—É–∑
        
        Args:
            clauses_df: DataFrame —Å –∫–ª–∞—É–∑–∞–º–∏
            start_idx: –ò–Ω–¥–µ–∫—Å –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–∑–º–µ—Ç–∫–∏
        """
        end_idx = min(start_idx + batch_size, len(clauses_df))
        batch_df = clauses_df.iloc[start_idx:end_idx]
        
        results = []
        total_clauses = len(batch_df)
        
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {total_clauses} –∫–ª–∞—É–∑ (–∏–Ω–¥–µ–∫—Å—ã {start_idx}-{end_idx-1})")
        
        for idx, (_, row) in enumerate(batch_df.iterrows()):
            try:
                logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª–∞—É–∑—É {idx + 1}/{total_clauses} (ID: {row['clause_id']})")
                
                result = self.process_clause(row)
                results.append(result)
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: topics={result['topics']}, sentiments={result['sentiments']}")
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–ª–∞—É–∑—ã {row['clause_id']}: {e}")
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
                results.append({
                    "clause_id": row['clause_id'],
                    "review_id": row['review_id'],
                    "clause": row['clause'],
                    "topics": [],
                    "sentiments": []
                })
        
        return results

def load_clauses_data(file_path: str) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–ª–∞—É–∑ –∏–∑ CSV —Ñ–∞–π–ª–∞
    
    Args:
        file_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
        
    Returns:
        DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—É–∑
    """
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {file_path}")
    
    try:
        df = pd.read_csv(file_path)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∫–ª–∞—É–∑")
        logger.info(f"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
        return df
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def save_results(results: List[Dict[str, Any]], output_path: str):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON —Ñ–∞–π–ª
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–∑–º–µ—Ç–∫–∏
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    logger.info(f"–°–æ—Ö—Ä–∞–Ω—è–µ–º {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_path}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    logger.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

def create_summary_stats(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —Ä–∞–∑–º–µ—Ç–∫–∏
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–∑–º–µ—Ç–∫–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    total_clauses = len(results)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–æ–ø–∏–∫–∞–º
    all_topics = []
    for result in results:
        all_topics.extend(result['topics'])
    topic_counts = pd.Series(all_topics).value_counts().to_dict()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—è–º
    all_sentiments = []
    for result in results:
        all_sentiments.extend(result['sentiments'])
    sentiment_counts = pd.Series(all_sentiments).value_counts().to_dict()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–ø–∏–∫–æ–≤/—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π –Ω–∞ –∫–ª–∞—É–∑—É
    topics_per_clause = [len(result['topics']) for result in results]
    sentiments_per_clause = [len(result['sentiments']) for result in results]
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
    clauses_with_context = sum(1 for result in results if result.get('has_full_context', False))
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π
    equal_counts = sum(1 for result in results if len(result['topics']) == len(result['sentiments']))
    
    stats = {
        "total_clauses": total_clauses,
        "clauses_with_topics": sum(1 for result in results if result['topics']),
        "clauses_with_sentiments": sum(1 for result in results if result['sentiments']),
        "clauses_with_full_context": clauses_with_context,
        "clauses_with_equal_counts": equal_counts,
        "equal_counts_percentage": (equal_counts / total_clauses * 100) if total_clauses > 0 else 0,
        "topic_distribution": topic_counts,
        "sentiment_distribution": sentiment_counts,
        "avg_topics_per_clause": sum(topics_per_clause) / len(topics_per_clause) if topics_per_clause else 0,
        "avg_sentiments_per_clause": sum(sentiments_per_clause) / len(sentiments_per_clause) if sentiments_per_clause else 0
    }
    
    return stats

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    project_root = Path(__file__).parent.parent.parent
    input_file = str(project_root / "data/interim/clauses.csv")
    output_file = str(project_root / "data/processed/labeling/clauses_labeled.json")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
    TEST_SIZE = None  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—É–∑ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–ª–µ–≥–∫–æ –∏–∑–º–µ–Ω–∏—Ç—å) None –¥–ª—è –≤—Å–µ—Ö
    
    try:
        # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞
        start_time = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        clauses_df = load_clauses_data(input_file)
        
        # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        labeler = OllamaClauseLabeler()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        merged_json_path = str(project_root / "data/raw/merged.json")
        labeler.load_review_texts(merged_json_path)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª–∞—É–∑—ã (–≤—Å–µ –µ—Å–ª–∏ TEST_SIZE=None)
        batch_size = len(clauses_df) if TEST_SIZE is None else TEST_SIZE
        results = labeler.process_clauses_batch(clauses_df, start_idx=0, batch_size=batch_size)
        
        # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è
        end_time = time.time()
        total_time = end_time - start_time
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = create_summary_stats(results)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        stats["execution_time_seconds"] = total_time
        stats["execution_time_minutes"] = total_time / 60
        stats["avg_time_per_clause"] = total_time / len(results) if results else 0
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        save_results(results, output_file)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ reports/labeling
        stats_file = str(project_root / "reports/labeling/clauses_labeled_stats.json")
        os.makedirs(os.path.dirname(stats_file), exist_ok=True)
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        logger.info("=" * 60)
        logger.info("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        logger.info("=" * 60)
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–ª–∞—É–∑: {stats['total_clauses']}")
        logger.info(f"–ö–ª–∞—É–∑ —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: {stats['clauses_with_full_context']} ({stats['clauses_with_full_context']/stats['total_clauses']*100:.1f}%)")
        logger.info(f"–ö–ª–∞—É–∑ —Å —Ç–æ–ø–∏–∫–∞–º–∏: {stats['clauses_with_topics']} ({stats['clauses_with_topics']/stats['total_clauses']*100:.1f}%)")
        logger.info(f"–ö–ª–∞—É–∑ —Å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—è–º–∏: {stats['clauses_with_sentiments']} ({stats['clauses_with_sentiments']/stats['total_clauses']*100:.1f}%)")
        logger.info(f"–ö–ª–∞—É–∑ —Å —Ä–∞–≤–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤/—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π: {stats['clauses_with_equal_counts']} ({stats['equal_counts_percentage']:.1f}%)")
        logger.info(f"–°—Ä–µ–¥–Ω–µ–µ —Ç–æ–ø–∏–∫–æ–≤ –Ω–∞ –∫–ª–∞—É–∑—É: {stats['avg_topics_per_clause']:.2f}")
        logger.info(f"–°—Ä–µ–¥–Ω–µ–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π –Ω–∞ –∫–ª–∞—É–∑—É: {stats['avg_sentiments_per_clause']:.2f}")
        logger.info(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {stats['execution_time_minutes']:.1f} –º–∏–Ω—É—Ç")
        logger.info(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –∫–ª–∞—É–∑—É: {stats['avg_time_per_clause']:.2f} —Å–µ–∫—É–Ω–¥")
        
        logger.info("\n–¢–æ–ø-5 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã—Ö —Ç–æ–ø–∏–∫–æ–≤:")
        for topic, count in list(stats['topic_distribution'].items())[:5]:
            logger.info(f"  {topic}: {count}")
        
        logger.info("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π:")
        for sentiment, count in stats['sentiment_distribution'].items():
            logger.info(f"  {sentiment}: {count}")
        
        logger.info(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {stats_file}")
        logger.info(f"üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:")
        logger.info(f"   - –î–∞–Ω–Ω—ã–µ: {project_root}/data/processed/labeling/")
        logger.info(f"   - –û—Ç—á–µ—Ç—ã: {project_root}/reports/labeling/")
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise

if __name__ == "__main__":
    main()
