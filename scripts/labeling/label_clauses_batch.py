#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—É–∑ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–ª–∞—É–∑—ã –ø–æ—Ä—Ü–∏—è–º–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —Å —Ç–æ–≥–æ –º–µ—Å—Ç–∞, –≥–¥–µ –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∏—Å—å.
"""

import pandas as pd
import json
import os
import argparse
from pathlib import Path
from label_clauses_ollama import OllamaClauseLabeler, create_summary_stats
import logging
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BatchClauseProcessor:
    def __init__(self, input_file: str, output_dir: str, model_name: str = "llama3.1:8b-instruct-q8_0"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            input_file: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama
        """
        self.input_file = input_file
        self.output_dir = Path(output_dir)
        self.model_name = model_name
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.batches_dir = self.output_dir / "batches"
        self.batches_dir.mkdir(exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.labeler = OllamaClauseLabeler(model_name)
        
        # –§–∞–π–ª—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.progress_file = self.output_dir / "progress.json"
        self.final_output = self.output_dir / "clauses_labeled.json"
        self.stats_output = self.output_dir / "clauses_labeled_stats.json"
    
    def load_progress(self) -> dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ"""
        if self.progress_file.exists():
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"processed_batches": [], "last_batch_idx": -1, "total_processed": 0}
    
    def save_progress(self, progress: dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ"""
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    
    def process_batch(self, clauses_df: pd.DataFrame, batch_idx: int, 
                     start_idx: int, batch_size: int) -> str:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞
        
        Args:
            clauses_df: DataFrame —Å –∫–ª–∞—É–∑–∞–º–∏
            batch_idx: –ù–æ–º–µ—Ä –±–∞—Ç—á–∞
            start_idx: –ù–∞—á–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            
        Returns:
            –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –±–∞—Ç—á–∞
        """
        batch_file = self.batches_dir / f"batch_{batch_idx:04d}.json"
        
        # –ï—Å–ª–∏ –±–∞—Ç—á —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if batch_file.exists():
            logger.info(f"–ë–∞—Ç—á {batch_idx} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return str(batch_file)
        
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á {batch_idx} (–∏–Ω–¥–µ–∫—Å—ã {start_idx}-{start_idx + batch_size - 1})")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á
        results = self.labeler.process_clauses_batch(clauses_df, start_idx, batch_size)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞—Ç—á–∞
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"–ë–∞—Ç—á {batch_idx} —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {batch_file}")
        return str(batch_file)
    
    def merge_batches(self, batch_files: list) -> list:
        """
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—Å–µ—Ö –±–∞—Ç—á–µ–π
        
        Args:
            batch_files: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º –±–∞—Ç—á–µ–π
            
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        logger.info("–û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –±–∞—Ç—á–µ–π...")
        
        all_results = []
        for batch_file in batch_files:
            if os.path.exists(batch_file):
                with open(batch_file, 'r', encoding='utf-8') as f:
                    batch_results = json.load(f)
                    all_results.extend(batch_results)
        
        logger.info(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return all_results
    
    def process_all_clauses(self, batch_size: int = 100, max_clauses: int = None):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∫–ª–∞—É–∑ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –±–∞—Ç—á–∏
        
        Args:
            batch_size: –†–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞
            max_clauses: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—É–∑ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (None = –≤—Å–µ)
        """
        start_time = time.time()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {self.input_file}")
        clauses_df = pd.read_csv(self.input_file)
        
        if max_clauses:
            clauses_df = clauses_df.head(max_clauses)
            logger.info(f"–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ {max_clauses} –∫–ª–∞—É–∑")
        
        total_clauses = len(clauses_df)
        total_batches = (total_clauses + batch_size - 1) // batch_size
        
        logger.info(f"–í—Å–µ–≥–æ –∫–ª–∞—É–∑: {total_clauses}")
        logger.info(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
        logger.info(f"–í—Å–µ–≥–æ –±–∞—Ç—á–µ–π: {total_batches}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        progress = self.load_progress()
        start_batch = progress["last_batch_idx"] + 1
        
        if start_batch > 0:
            logger.info(f"–í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–±–æ—Ç—É —Å –±–∞—Ç—á–∞ {start_batch}")
        
        batch_files = []
        
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∏
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                current_batch_size = min(batch_size, total_clauses - start_idx)
                
                if batch_idx < start_batch:
                    # –ë–∞—Ç—á —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –≤ —Å–ø–∏—Å–æ–∫
                    batch_file = self.batches_dir / f"batch_{batch_idx:04d}.json"
                    if batch_file.exists():
                        batch_files.append(str(batch_file))
                    continue
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π –±–∞—Ç—á
                batch_file = self.process_batch(clauses_df, batch_idx, start_idx, current_batch_size)
                batch_files.append(batch_file)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress["processed_batches"].append(batch_idx)
                progress["last_batch_idx"] = batch_idx
                progress["total_processed"] += current_batch_size
                self.save_progress(progress)
                
                logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {progress['total_processed']}/{total_clauses} –∫–ª–∞—É–∑ ({progress['total_processed']/total_clauses*100:.1f}%)")
        
        except KeyboardInterrupt:
            logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")
            return
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
            return
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_results = self.merge_batches(batch_files)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª
        with open(self.final_output, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = create_summary_stats(all_results)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        stats["execution_time_seconds"] = total_time
        stats["execution_time_minutes"] = total_time / 60
        stats["avg_time_per_clause"] = total_time / len(all_results) if all_results else 0
        stats["batch_size"] = batch_size
        stats["total_batches"] = total_batches
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ reports/labeling
        project_root = Path(__file__).parent.parent.parent
        stats_output = project_root / "reports/labeling/batch_clauses_labeled_stats.json"
        os.makedirs(stats_output.parent, exist_ok=True)
        
        with open(stats_output, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        logger.info("=" * 60)
        logger.info("–û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
        logger.info("=" * 60)
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–ª–∞—É–∑: {len(all_results)}")
        logger.info(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
        logger.info(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –∫–ª–∞—É–∑—É: {total_time/len(all_results):.2f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {self.final_output}")
        logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats_output}")
        logger.info(f"üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:")
        logger.info(f"   - –î–∞–Ω–Ω—ã–µ: {project_root}/data/processed/labeling/")
        logger.info(f"   - –û—Ç—á–µ—Ç—ã: {project_root}/reports/labeling/")
        
        # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        if self.progress_file.exists():
            self.progress_file.unlink()
            logger.info("–§–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ—á–∏—â–µ–Ω")

def main():
    parser = argparse.ArgumentParser(description='–ü–∞–∫–µ—Ç–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –∫–ª–∞—É–∑ —Å –ø–æ–º–æ—â—å—é Ollama')
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞
    project_root = Path(__file__).parent.parent.parent
    
    parser.add_argument('--input', 
                       default=str(project_root / 'data/interim/clauses.csv'),
                       help='–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É')
    parser.add_argument('--output-dir',
                       default=str(project_root / 'data/processed/labeling'),
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('--batch-size', type=int, default=50,
                       help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--max-clauses', type=int, default=1000,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—É–∑ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (None = –≤—Å–µ)')
    parser.add_argument('--model', default='llama3.1:8b-instruct-q8_0',
                       help='–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Ollama')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    processor = BatchClauseProcessor(args.input, args.output_dir, args.model)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –æ—Ç–∑—ã–≤–æ–≤
    merged_json_path = project_root / "data/raw/merged.json"
    if merged_json_path.exists():
        processor.labeler.load_review_texts(str(merged_json_path))
    else:
        logger.warning("–§–∞–π–ª merged.json –Ω–µ –Ω–∞–π–¥–µ–Ω, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    processor.process_all_clauses(args.batch_size, args.max_clauses)

if __name__ == "__main__":
    main()
