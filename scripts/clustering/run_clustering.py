#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç-–ª–∞—É–Ω—á–µ—Ä –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
–∏ —Å–æ–∑–¥–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.
"""

import os
import sys
import argparse
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

def main():
    parser = argparse.ArgumentParser(description='–ó–∞–ø—É—Å–∫ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –æ—Ç–∑—ã–≤–æ–≤')
    parser.add_argument('--data-path', 
                       default=str(project_root / 'data/raw/merged (1).json'),
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏')
    parser.add_argument('--methods', 
                       nargs='+',
                       choices=['embedding', 'topic', 'tfidf', 'all'],
                       default=['all'],
                       help='–ú–µ—Ç–æ–¥—ã –¥–ª—è –∑–∞–ø—É—Å–∫–∞')
    parser.add_argument('--quick', 
                       action='store_true',
                       help='–ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞)')
    parser.add_argument('--no-viz', 
                       action='store_true',
                       help='–û—Ç–∫–ª—é—á–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π')
    
    args = parser.parse_args()
    
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –æ—Ç–∑—ã–≤–æ–≤ –±–∞–Ω–∫–∞")
    print("=" * 60)
    print(f"üìÅ –î–∞–Ω–Ω—ã–µ: {args.data_path}")
    print(f"üîß –ú–µ—Ç–æ–¥—ã: {args.methods}")
    print(f"‚ö° –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º: {'–î–∞' if args.quick else '–ù–µ—Ç'}")
    print(f"üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {'–ù–µ—Ç' if args.no_viz else '–î–∞'}")
    print()
    
    # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–ø–∫–∏
    os.makedirs(project_root / 'data/processed/clustering', exist_ok=True)
    os.makedirs(project_root / 'reports/clustering', exist_ok=True)
    
    methods_to_run = args.methods if 'all' not in args.methods else ['embedding', 'topic', 'tfidf']
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    if 'embedding' in methods_to_run:
        print("üß† –ó–∞–ø—É—Å–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        run_embedding_clustering(args.data_path, args.quick, args.no_viz)
    
    if 'topic' in methods_to_run:
        print("üìù –ó–∞–ø—É—Å–∫ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è...")
        run_topic_modeling(args.data_path, args.quick, args.no_viz)
    
    if 'tfidf' in methods_to_run:
        print("üìä –ó–∞–ø—É—Å–∫ TF-IDF –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
        run_tfidf_clustering(args.data_path, args.quick, args.no_viz)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if len(methods_to_run) > 1:
        print("üìà –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        run_evaluation(args.data_path)
    
    print("\n‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
    print(f"   - –î–∞–Ω–Ω—ã–µ: {project_root}/data/processed/clustering/")
    print(f"   - –û—Ç—á–µ—Ç—ã: {project_root}/reports/clustering/")

def run_embedding_clustering(data_path, quick=False, no_viz=False):
    """–ó–∞–ø—É—Å–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    try:
        from embedding_clustering import EmbeddingClustering
        
        clustering = EmbeddingClustering(data_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –ø–∞–º—è—Ç—å—é
        max_samples = 1000 if quick else 10000
        clustering.load_data(max_samples=max_samples)
        
        if quick:
            print("  ‚ö° –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ 1000 –æ—Ç–∑—ã–≤–æ–≤")
        else:
            print("  üìä –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ 10000 –æ—Ç–∑—ã–≤–æ–≤")
        
        clustering.load_model('cointegrated/rubert-tiny2')  # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
        clustering.create_embeddings(batch_size=16)  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        clustering.perform_clustering()
        clustering.analyze_clusters()
        
        if not no_viz:
            clustering.visualize_clusters()
        
        output_path = str(project_root / 'data/processed/clustering/embedding_clustering_results.json')
        clustering.save_results(output_path)
        
        print("  ‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}")
        import traceback
        print(f"  –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏: {traceback.format_exc()}")

def run_topic_modeling(data_path, quick=False, no_viz=False):
    """–ó–∞–ø—É—Å–∫ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è"""
    try:
        from topic_modeling import TopicModeling
        
        topic_modeling = TopicModeling(data_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –ø–∞–º—è—Ç—å—é
        max_samples = 1000 if quick else 40000
        topic_modeling.load_data(max_samples=max_samples)
        
        if quick:
            print("  ‚ö° –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ 1000 –æ—Ç–∑—ã–≤–æ–≤")
        else:
            print("  üìä –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ 10000 –æ—Ç–∑—ã–≤–æ–≤")
        
        topic_modeling.prepare_texts()
        
        # LDA —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –¥–∏–∞–ø–∞–∑–æ–Ω–æ–º –≤ –±—ã—Å—Ç—Ä–æ–º —Ä–µ–∂–∏–º–µ
        topics_range = (3, 10) if quick else (5, 20)
        lda_results = topic_modeling.lda_modeling(n_topics_range=topics_range)
        
        # BERTopic (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤ –±—ã—Å—Ç—Ä–æ–º —Ä–µ–∂–∏–º–µ –∏–∑-–∑–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ —Ä–µ—Å—É—Ä—Å–∞–º)
        if not quick:
            bertopic_results = topic_modeling.bertopic_modeling()
            topic_modeling.compare_methods(lda_results, bertopic_results)
        else:
            bertopic_results = None
            print("  ‚ö° BERTopic –ø—Ä–æ–ø—É—â–µ–Ω –≤ –±—ã—Å—Ç—Ä–æ–º —Ä–µ–∂–∏–º–µ")
        
        output_path = str(project_root / 'data/processed/clustering/topic_modeling_results.json')
        if bertopic_results:
            topic_modeling.save_results(lda_results, bertopic_results, output_path)
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ LDA —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            topic_modeling.df.to_json(output_path, orient='records', indent=2)
        
        print("  ‚úÖ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback
        print(f"  –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏: {traceback.format_exc()}")

def run_tfidf_clustering(data_path, quick=False, no_viz=False):
    """–ó–∞–ø—É—Å–∫ TF-IDF –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    try:
        from tfidf_clustering import TfIdfClustering
        
        clustering = TfIdfClustering(data_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å –ø–∞–º—è—Ç—å—é
        max_samples = 1000 if quick else 40000
        clustering.load_data(max_samples=max_samples)
        
        if quick:
            print("  ‚ö° –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ 1000 –æ—Ç–∑—ã–≤–æ–≤")
        else:
            print("  üìä –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ 10000 –æ—Ç–∑—ã–≤–æ–≤")
        
        clustering.prepare_texts()
        
        # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–∂–∏–º–∞
        max_features = 1000 if quick else 5000
        clustering.create_tfidf_matrix(max_features=max_features)
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –≤ –±—ã—Å—Ç—Ä–æ–º —Ä–µ–∂–∏–º–µ
        algorithms = ['kmeans'] if quick else ['kmeans', 'agglomerative', 'dbscan']
        results = clustering.perform_clustering(algorithms=algorithms)
        
        if not no_viz:
            if not quick:
                clustering.create_word_clouds(results)
            clustering.visualize_clusters(results)
            clustering.compare_algorithms(results)
        
        output_path = str(project_root / 'data/processed/clustering/tfidf_clustering_results.json')
        clustering.save_results(results, output_path)
        
        print("  ‚úÖ TF-IDF –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –≤ TF-IDF –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        import traceback
        print(f"  –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏: {traceback.format_exc()}")

def run_evaluation(data_path):
    """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏"""
    try:
        from clustering_evaluation import ClusteringEvaluation
        
        results_paths = {
            'embedding_clustering': str(project_root / 'data/processed/clustering/embedding_clustering_results.json'),
            'topic_modeling': str(project_root / 'data/processed/clustering/topic_modeling_results.json'),
            'tfidf_clustering': str(project_root / 'data/processed/clustering/tfidf_clustering_results.json')
        }
        
        evaluation = ClusteringEvaluation(data_path)
        evaluation.load_original_data()
        evaluation.load_clustering_results(results_paths)
        
        # –ê–Ω–∞–ª–∏–∑—ã
        consistency_results = evaluation.analyze_cluster_consistency()
        alignment_results = evaluation.analyze_product_type_alignment()
        profiles = evaluation.create_cluster_profiles()
        quality_metrics = evaluation.evaluate_clustering_quality()
        
        evaluation.create_summary_visualization(quality_metrics)
        recommendations = evaluation.generate_recommendations(quality_metrics, alignment_results)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        output_path = str(project_root / 'reports/clustering/clustering_evaluation_report.txt')
        evaluation.save_evaluation_report(
            quality_metrics, alignment_results, recommendations, output_path
        )
        
        print("  ‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–µ: {e}")

if __name__ == "__main__":
    main()
