#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–û—Ü–µ–Ω–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
–∏ —Å–æ–∑–¥–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import warnings
warnings.filterwarnings('ignore')

# –î–ª—è –º–µ—Ç—Ä–∏–∫
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

class ClusteringEvaluation:
    def __init__(self, data_path):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        
        Args:
            data_path (str): –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        """
        self.data_path = data_path
        self.original_data = None
        self.results_data = {}
        self.evaluation_metrics = {}
        
    def load_original_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        with open(self.data_path, 'r', encoding='utf-8') as f:
            self.original_data = pd.DataFrame(json.load(f))
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.original_data)} –æ—Ç–∑—ã–≤–æ–≤")
    
    def load_clustering_results(self, results_paths):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        
        Args:
            results_paths (dict): –°–ª–æ–≤–∞—Ä—å —Å –ø—É—Ç—è–º–∏ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
        """
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
        
        for method_name, path in results_paths.items():
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.results_data[method_name] = pd.DataFrame(data)
                print(f"  {method_name}: {len(data)} –∑–∞–ø–∏—Å–µ–π")
            except FileNotFoundError:
                print(f"  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Ñ–∞–π–ª {path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                continue
    
    def analyze_cluster_consistency(self):
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏"""
        print("\n–ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏:")
        print("=" * 50)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        clustering_columns = []
        common_df = None
        
        for method_name, df in self.results_data.items():
            # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫–∏ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            cluster_cols = [col for col in df.columns if 'cluster' in col.lower() or 'topic' in col.lower()]
            
            if cluster_cols:
                if common_df is None:
                    common_df = df[['review_id'] + cluster_cols].copy()
                    for col in cluster_cols:
                        new_col = f"{method_name}_{col}"
                        common_df[new_col] = common_df[col]
                        clustering_columns.append(new_col)
                else:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ review_id
                    temp_df = df[['review_id'] + cluster_cols].copy()
                    for col in cluster_cols:
                        new_col = f"{method_name}_{col}"
                        temp_df[new_col] = temp_df[col]
                    common_df = common_df.merge(temp_df[['review_id', new_col]], on='review_id', how='inner')
                    clustering_columns.append(new_col)
        
        if common_df is None or len(clustering_columns) < 2:
            print("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤")
            return None
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        consistency_matrix = np.zeros((len(clustering_columns), len(clustering_columns)))
        
        for i, method1 in enumerate(clustering_columns):
            for j, method2 in enumerate(clustering_columns):
                if i != j:
                    # –£–±–∏—Ä–∞–µ–º –∑–∞–ø–∏—Å–∏ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                    mask = (common_df[method1].notna()) & (common_df[method2].notna())
                    if mask.sum() > 0:
                        labels1 = common_df.loc[mask, method1]
                        labels2 = common_df.loc[mask, method2]
                        
                        # Adjusted Rand Index
                        ari = adjusted_rand_score(labels1, labels2)
                        consistency_matrix[i, j] = ari
                else:
                    consistency_matrix[i, j] = 1.0
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        plt.figure(figsize=(10, 8))
        sns.heatmap(consistency_matrix, 
                   annot=True, 
                   fmt='.3f',
                   xticklabels=[col.replace('_', '\n') for col in clustering_columns],
                   yticklabels=[col.replace('_', '\n') for col in clustering_columns],
                   cmap='viridis',
                   vmin=0, vmax=1)
        plt.title('–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏\n(Adjusted Rand Index)')
        plt.tight_layout()
        plt.savefig('/Users/mishantique/Desktop/Projects/gazprombank_hachaton/reports/clustering/clustering_consistency_matrix.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ä–µ–¥–Ω—é—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
        print("\n–°—Ä–µ–¥–Ω—è—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤ —Å –¥—Ä—É–≥–∏–º–∏:")
        for i, method in enumerate(clustering_columns):
            avg_consistency = np.mean([consistency_matrix[i, j] for j in range(len(clustering_columns)) if i != j])
            print(f"  {method}: {avg_consistency:.3f}")
        
        return consistency_matrix, clustering_columns
    
    def analyze_product_type_alignment(self):
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Ä–µ–∞–ª—å–Ω—ã–º —Ç–∏–ø–∞–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤"""
        print("\n–ê–Ω–∞–ª–∏–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Ç–∏–ø–∞–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤:")
        print("=" * 50)
        
        alignment_results = {}
        
        for method_name, df in self.results_data.items():
            print(f"\n{method_name.upper()}:")
            
            # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫–∏ —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
            cluster_cols = [col for col in df.columns if 'cluster' in col.lower() or 'topic' in col.lower()]
            
            for cluster_col in cluster_cols:
                if cluster_col in df.columns and 'product_type' in df.columns:
                    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å–æ–ø—Ä—è–∂–µ–Ω–Ω–æ—Å—Ç–∏
                    contingency_table = pd.crosstab(df[cluster_col], df['product_type'])
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º —á–∏—Å—Ç–æ—Ç—É –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    cluster_purities = []
                    for cluster_id in contingency_table.index:
                        if cluster_id != -1:  # –ò—Å–∫–ª—é—á–∞–µ–º outliers
                            cluster_row = contingency_table.loc[cluster_id]
                            purity = cluster_row.max() / cluster_row.sum()
                            cluster_purities.append(purity)
                            
                            # –ù–∞—Ö–æ–¥–∏–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π —Ç–∏–ø –ø—Ä–æ–¥—É–∫—Ç–∞
                            dominant_product = cluster_row.idxmax()
                            print(f"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {purity:.3f} —á–∏—Å—Ç–æ—Ç—ã, –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç '{dominant_product}'")
                    
                    avg_purity = np.mean(cluster_purities) if cluster_purities else 0
                    alignment_results[f"{method_name}_{cluster_col}"] = {
                        'avg_purity': avg_purity,
                        'contingency_table': contingency_table,
                        'cluster_purities': cluster_purities
                    }
                    
                    print(f"  –°—Ä–µ–¥–Ω—è—è —á–∏—Å—Ç–æ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {avg_purity:.3f}")
        
        return alignment_results
    
    def create_cluster_profiles(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞"""
        print("\n–°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
        
        profiles = {}
        
        for method_name, df in self.results_data.items():
            print(f"\n–ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ - {method_name.upper()}:")
            print("-" * 30)
            
            method_profiles = {}
            cluster_cols = [col for col in df.columns if 'cluster' in col.lower() or 'topic' in col.lower()]
            
            for cluster_col in cluster_cols[:1]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
                if cluster_col not in df.columns:
                    continue
                    
                unique_clusters = sorted(df[cluster_col].unique())
                
                for cluster_id in unique_clusters:
                    if cluster_id == -1:
                        continue
                        
                    cluster_data = df[df[cluster_col] == cluster_id]
                    
                    # –ü—Ä–æ—Ñ–∏–ª—å –∫–ª–∞—Å—Ç–µ—Ä–∞
                    profile = {
                        'size': len(cluster_data),
                        'percentage': len(cluster_data) / len(df) * 100,
                        'top_products': cluster_data['product_type'].value_counts().head(3).to_dict(),
                        'avg_text_length': cluster_data['review_text'].str.len().mean(),
                        'date_range': {
                            'min': cluster_data['review_date'].min(),
                            'max': cluster_data['review_date'].max()
                        } if 'review_date' in cluster_data.columns else None
                    }
                    
                    method_profiles[cluster_id] = profile
                    
                    print(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}:")
                    print(f"  –†–∞–∑–º–µ—Ä: {profile['size']} ({profile['percentage']:.1f}%)")
                    print(f"  –¢–æ–ø –ø—Ä–æ–¥—É–∫—Ç—ã: {profile['top_products']}")
                    print(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {profile['avg_text_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            
            profiles[method_name] = method_profiles
        
        return profiles
    
    def evaluate_clustering_quality(self):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤"""
        print("\n–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
        print("=" * 50)
        
        quality_metrics = {}
        
        for method_name, df in self.results_data.items():
            print(f"\n{method_name.upper()}:")
            
            method_metrics = {}
            cluster_cols = [col for col in df.columns if 'cluster' in col.lower() or 'topic' in col.lower()]
            
            for cluster_col in cluster_cols:
                if cluster_col not in df.columns:
                    continue
                
                clusters = df[cluster_col].values
                unique_clusters = set(clusters)
                
                # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                n_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)
                n_outliers = sum(1 for c in clusters if c == -1)
                
                # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                cluster_sizes = [sum(1 for c in clusters if c == cluster_id) 
                               for cluster_id in unique_clusters if cluster_id != -1]
                
                if cluster_sizes:
                    size_std = np.std(cluster_sizes)
                    size_cv = size_std / np.mean(cluster_sizes) if np.mean(cluster_sizes) > 0 else 0
                else:
                    size_std = size_cv = 0
                
                # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–∏–ø–∞–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤ (—á–∏—Å—Ç–æ—Ç–∞)
                if 'product_type' in df.columns:
                    cluster_purities = []
                    for cluster_id in unique_clusters:
                        if cluster_id != -1:
                            cluster_mask = clusters == cluster_id
                            if cluster_mask.sum() > 0:
                                cluster_products = df.loc[cluster_mask, 'product_type']
                                purity = cluster_products.value_counts().max() / len(cluster_products)
                                cluster_purities.append(purity)
                    
                    avg_purity = np.mean(cluster_purities) if cluster_purities else 0
                else:
                    avg_purity = 0
                
                method_metrics[cluster_col] = {
                    'n_clusters': n_clusters,
                    'n_outliers': n_outliers,
                    'outlier_percentage': n_outliers / len(clusters) * 100,
                    'avg_cluster_size': np.mean(cluster_sizes) if cluster_sizes else 0,
                    'cluster_size_std': size_std,
                    'cluster_size_cv': size_cv,
                    'avg_purity': avg_purity
                }
                
                print(f"  {cluster_col}:")
                print(f"    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
                print(f"    Outliers: {n_outliers} ({n_outliers / len(clusters) * 100:.1f}%)")
                print(f"    –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞: {np.mean(cluster_sizes):.1f}")
                print(f"    –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–æ–≤: {size_cv:.3f}")
                print(f"    –°—Ä–µ–¥–Ω—è—è —á–∏—Å—Ç–æ—Ç–∞: {avg_purity:.3f}")
            
            quality_metrics[method_name] = method_metrics
        
        return quality_metrics
    
    def create_summary_visualization(self, quality_metrics):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–µ—Ç–æ–¥–æ–≤"""
        print("\n–°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é...")
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        methods = []
        n_clusters_list = []
        avg_purity_list = []
        outlier_percentage_list = []
        size_cv_list = []
        
        for method_name, method_data in quality_metrics.items():
            for cluster_col, metrics in method_data.items():
                methods.append(f"{method_name}\n{cluster_col}")
                n_clusters_list.append(metrics['n_clusters'])
                avg_purity_list.append(metrics['avg_purity'])
                outlier_percentage_list.append(metrics['outlier_percentage'])
                size_cv_list.append(metrics['cluster_size_cv'])
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        bars1 = ax1.bar(range(len(methods)), n_clusters_list)
        ax1.set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
        ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
        ax1.set_xticks(range(len(methods)))
        ax1.set_xticklabels(methods, rotation=45, ha='right')
        
        # –°—Ä–µ–¥–Ω—è—è —á–∏—Å—Ç–æ—Ç–∞
        bars2 = ax2.bar(range(len(methods)), avg_purity_list, color='green', alpha=0.7)
        ax2.set_title('–°—Ä–µ–¥–Ω—è—è —á–∏—Å—Ç–æ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
        ax2.set_ylabel('–ß–∏—Å—Ç–æ—Ç–∞')
        ax2.set_ylim(0, 1)
        ax2.set_xticks(range(len(methods)))
        ax2.set_xticklabels(methods, rotation=45, ha='right')
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç outliers
        bars3 = ax3.bar(range(len(methods)), outlier_percentage_list, color='red', alpha=0.7)
        ax3.set_title('–ü—Ä–æ—Ü–µ–Ω—Ç outliers')
        ax3.set_ylabel('–ü—Ä–æ—Ü–µ–Ω—Ç (%)')
        ax3.set_xticks(range(len(methods)))
        ax3.set_xticklabels(methods, rotation=45, ha='right')
        
        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        bars4 = ax4.bar(range(len(methods)), size_cv_list, color='orange', alpha=0.7)
        ax4.set_title('–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
        ax4.set_ylabel('–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏')
        ax4.set_xticks(range(len(methods)))
        ax4.set_xticklabels(methods, rotation=45, ha='right')
        
        plt.tight_layout()
        plt.savefig('/Users/mishantique/Desktop/Projects/gazprombank_hachaton/reports/clustering/clustering_quality_comparison.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_recommendations(self, quality_metrics, alignment_results):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –≤—ã–±–æ—Ä—É –º–µ—Ç–æ–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É –º–µ—Ç–æ–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
        print("=" * 50)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–µ—Ç–æ–¥—ã –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        method_scores = {}
        
        for method_name, method_data in quality_metrics.items():
            for cluster_col, metrics in method_data.items():
                method_key = f"{method_name}_{cluster_col}"
                
                # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç 0 –¥–æ 1)
                purity_score = metrics['avg_purity']  # –£–∂–µ –æ—Ç 0 –¥–æ 1
                
                # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 10-20 –∫–∞–∫ –æ–ø—Ç–∏–º—É–º)
                n_clusters = metrics['n_clusters']
                if 8 <= n_clusters <= 25:
                    cluster_count_score = 1.0
                elif 5 <= n_clusters < 8 or 25 < n_clusters <= 30:
                    cluster_count_score = 0.7
                else:
                    cluster_count_score = 0.3
                
                # –ü—Ä–æ—Ü–µ–Ω—Ç outliers (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
                outlier_score = max(0, 1 - metrics['outlier_percentage'] / 50)  # 50% outliers = 0 score
                
                # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–º–µ–Ω—å—à–µ CV = –ª—É—á—à–µ)
                balance_score = max(0, 1 - metrics['cluster_size_cv'])
                
                # –û–±—â–∏–π —Å—á–µ—Ç
                total_score = (purity_score * 0.4 + 
                             cluster_count_score * 0.25 + 
                             outlier_score * 0.2 + 
                             balance_score * 0.15)
                
                method_scores[method_key] = {
                    'total_score': total_score,
                    'purity_score': purity_score,
                    'cluster_count_score': cluster_count_score,
                    'outlier_score': outlier_score,
                    'balance_score': balance_score,
                    'metrics': metrics
                }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç–æ–¥—ã –ø–æ –æ–±—â–µ–º—É —Å—á–µ—Ç—É
        sorted_methods = sorted(method_scores.items(), key=lambda x: x[1]['total_score'], reverse=True)
        
        print("–†–µ–π—Ç–∏–Ω–≥ –º–µ—Ç–æ–¥–æ–≤ (–æ—Ç –ª—É—á—à–µ–≥–æ –∫ —Ö—É–¥—à–µ–º—É):")
        for i, (method_key, scores) in enumerate(sorted_methods):
            print(f"\n{i+1}. {method_key}:")
            print(f"   –û–±—â–∏–π —Å—á–µ—Ç: {scores['total_score']:.3f}")
            print(f"   –ß–∏—Å—Ç–æ—Ç–∞: {scores['purity_score']:.3f}")
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {scores['metrics']['n_clusters']}")
            print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç outliers: {scores['metrics']['outlier_percentage']:.1f}%")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
        print(f"\nüèÜ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ô –ú–ï–¢–û–î: {sorted_methods[0][0]}")
        best_method = sorted_methods[0]
        
        print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é:")
        
        if best_method[1]['purity_score'] > 0.7:
            print("‚úÖ –í—ã—Å–æ–∫–∞—è —á–∏—Å—Ç–æ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ - –º–µ—Ç–æ–¥ —Ö–æ—Ä–æ—à–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Ç–∏–ø—ã –ø—Ä–æ–¥—É–∫—Ç–æ–≤")
        elif best_method[1]['purity_score'] > 0.5:
            print("‚ö†Ô∏è  –°—Ä–µ–¥–Ω—è—è —á–∏—Å—Ç–æ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ - –≤–æ–∑–º–æ–∂–Ω—ã —Å–º–µ—à–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã")
        else:
            print("‚ùå –ù–∏–∑–∫–∞—è —á–∏—Å—Ç–æ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ - –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–ª–æ—Ö–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç–∏–ø–∞–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤")
        
        if best_method[1]['metrics']['outlier_percentage'] < 10:
            print("‚úÖ –ù–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç outliers - –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ —É—Å–ø–µ—à–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω—ã")
        elif best_method[1]['metrics']['outlier_percentage'] < 25:
            print("‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç outliers - —á–∞—Å—Ç—å –æ—Ç–∑—ã–≤–æ–≤ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
        else:
            print("‚ùå –í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç outliers - –º–Ω–æ–≥–∏–µ –æ—Ç–∑—ã–≤—ã –Ω–µ –ø–æ–ø–∞–ª–∏ –≤ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã")
        
        optimal_clusters = best_method[1]['metrics']['n_clusters']
        if 10 <= optimal_clusters <= 20:
            print("‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏")
        elif optimal_clusters < 10:
            print("‚ö†Ô∏è  –ú–∞–ª–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ - –≤–æ–∑–º–æ–∂–Ω–∞ —á—Ä–µ–∑–º–µ—Ä–Ω–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è")
        else:
            print("‚ö†Ô∏è  –ë–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ - –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å")
        
        return sorted_methods
    
    def save_evaluation_report(self, quality_metrics, alignment_results, recommendations, output_path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –æ—Ü–µ–Ω–∫–µ"""
        print(f"\n–°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –≤ {output_path}...")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ü–û –û–¶–ï–ù–ö–ï –ú–ï–¢–û–î–û–í –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("–¶–ï–õ–¨ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:\n")
            f.write("–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤ –±–∞–Ω–∫–∞ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è\n")
            f.write("–æ—Å–Ω–æ–≤–Ω—ã—Ö –≥—Ä—É–ø–ø –æ–±—Å—É–∂–¥–∞–µ–º—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ —É—Å–ª—É–≥.\n\n")
            
            f.write("–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ù–´–ï –ú–ï–¢–û–î–´:\n")
            f.write("1. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (BERT/sentence-transformers)\n")
            f.write("2. –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (LDA/BERTopic)\n")
            f.write("3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF (K-means, Agglomerative, DBSCAN)\n\n")
            
            f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò:\n")
            f.write("-" * 30 + "\n\n")
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            for method_name, method_data in quality_metrics.items():
                f.write(f"{method_name.upper()}:\n")
                for cluster_col, metrics in method_data.items():
                    f.write(f"  {cluster_col}:\n")
                    f.write(f"    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {metrics['n_clusters']}\n")
                    f.write(f"    Outliers: {metrics['n_outliers']} ({metrics['outlier_percentage']:.1f}%)\n")
                    f.write(f"    –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞: {metrics['avg_cluster_size']:.1f}\n")
                    f.write(f"    –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–æ–≤: {metrics['cluster_size_cv']:.3f}\n")
                    f.write(f"    –°—Ä–µ–¥–Ω—è—è —á–∏—Å—Ç–æ—Ç–∞: {metrics['avg_purity']:.3f}\n")
                f.write("\n")
            
            f.write("–†–ï–ô–¢–ò–ù–ì –ú–ï–¢–û–î–û–í:\n")
            f.write("-" * 20 + "\n")
            for i, (method_key, scores) in enumerate(recommendations):
                f.write(f"{i+1}. {method_key} (–æ–±—â–∏–π —Å—á–µ—Ç: {scores['total_score']:.3f})\n")
                f.write(f"   - –ß–∏—Å—Ç–æ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {scores['purity_score']:.3f}\n")
                f.write(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {scores['metrics']['n_clusters']}\n")
                f.write(f"   - –ü—Ä–æ—Ü–µ–Ω—Ç outliers: {scores['metrics']['outlier_percentage']:.1f}%\n\n")
            
            f.write(f"–†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ô –ú–ï–¢–û–î: {recommendations[0][0]}\n\n")
            
            f.write("–í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\n")
            f.write("-" * 30 + "\n")
            f.write("1. –î–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ —Å –Ω–∞–∏–≤—ã—Å—à–∏–º —Ä–µ–π—Ç–∏–Ω–≥–æ–º.\n")
            f.write("2. –ü—Ä–∏ –≤—ã–±–æ—Ä–µ –º–µ—Ç–æ–¥–∞ —É—á–∏—Ç—ã–≤–∞–π—Ç–µ –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —á–∏—Å—Ç–æ—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º outliers.\n")
            f.write("3. –î–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 10-20 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.\n")
            f.write("4. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å outliers –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.\n\n")
            
            f.write("–ü–†–ò–ú–ï–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:\n")
            f.write("-" * 25 + "\n")
            f.write("- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤\n")
            f.write("- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –ø—Ä–æ–¥—É–∫—Ç–∞–º\n")
            f.write("- –í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –≤ –±–∞–Ω–∫–æ–≤—Å–∫–æ–º –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–∏\n")
            f.write("- –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏\n")
            f.write("- –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –ª–∏–Ω–µ–π–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏\n")
        
        print("–ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    data_path = '/Users/mishantique/Desktop/Projects/gazprombank_hachaton/data/raw/sravni_ru/sravni_ru.json'
    
    results_paths = {
        'embedding_clustering': '/Users/mishantique/Desktop/Projects/gazprombank_hachaton/data/processed/embedding_clustering_results.json',
        'topic_modeling': '/Users/mishantique/Desktop/Projects/gazprombank_hachaton/data/processed/topic_modeling_results.json',
        'tfidf_clustering': '/Users/mishantique/Desktop/Projects/gazprombank_hachaton/data/processed/tfidf_clustering_results.json'
    }
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    evaluation = ClusteringEvaluation(data_path)
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—Ü–µ–Ω–∫—É
    evaluation.load_original_data()
    evaluation.load_clustering_results(results_paths)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    consistency_results = evaluation.analyze_cluster_consistency()
    alignment_results = evaluation.analyze_product_type_alignment()
    profiles = evaluation.create_cluster_profiles()
    quality_metrics = evaluation.evaluate_clustering_quality()
    
    # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    evaluation.create_summary_visualization(quality_metrics)
    recommendations = evaluation.generate_recommendations(quality_metrics, alignment_results)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    output_path = '/Users/mishantique/Desktop/Projects/gazprombank_hachaton/reports/clustering_evaluation_report.txt'
    evaluation.save_evaluation_report(quality_metrics, alignment_results, recommendations, output_path)
    
    print("\n–û—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ reports/")

if __name__ == "__main__":
    main()
