#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü—Ä–æ—Å—Ç–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –æ—Ç–∑—ã–≤–æ–≤

–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from collections import Counter
import re
import warnings
warnings.filterwarnings('ignore')

def load_data(data_path, sample_size=1000):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –≤—ã–±–æ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {data_path}...")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    df = pd.DataFrame(data)
    
    # –ë–µ—Ä–µ–º –≤—ã–±–æ—Ä–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    if len(df) > sample_size:
        df = df.sample(n=sample_size, random_state=42)
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É –∏–∑ {sample_size} –æ—Ç–∑—ã–≤–æ–≤")
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –æ—Ç–∑—ã–≤–æ–≤")
    return df

def preprocess_text(text):
    """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return ""
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = text.lower()
    
    # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ —Ü–∏—Ñ—Ä—ã
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\d+', '', text)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def analyze_data(df):
    """–ê–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("\n=== –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• ===")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤
    product_counts = df['product_type'].value_counts()
    print(f"\n–¢–æ–ø-5 —Ç–∏–ø–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤:")
    for product, count in product_counts.head(5).items():
        print(f"  {product}: {count}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤
    text_lengths = df['review_text'].str.len()
    print(f"\n–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤:")
    print(f"  –°—Ä–µ–¥–Ω—è—è: {text_lengths.mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  –ú–µ–¥–∏–∞–Ω–∞: {text_lengths.median():.0f} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  –ú–∏–Ω–∏–º—É–º: {text_lengths.min()}")
    print(f"  –ú–∞–∫—Å–∏–º—É–º: {text_lengths.max()}")
    
    return product_counts

def simple_tfidf_clustering(df, n_clusters=8):
    """–ü—Ä–æ—Å—Ç–∞—è TF-IDF –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"""
    print(f"\n=== TF-IDF –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø ===")
    print(f"–¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    print("–ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã...")
    processed_texts = df['review_text'].apply(preprocess_text)
    
    # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
    valid_mask = processed_texts.str.len() > 10
    processed_texts = processed_texts[valid_mask].reset_index(drop=True)
    df_clean = df[valid_mask].copy().reset_index(drop=True)
    
    print(f"–ü–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(processed_texts)} —Ç–µ–∫—Å—Ç–æ–≤")
    
    # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    print("–°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä—ã...")
    vectorizer = TfidfVectorizer(
        max_features=1000,
        min_df=2,
        max_df=0.8,
        ngram_range=(1, 2),
        stop_words=None  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ –±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤
    )
    
    tfidf_matrix = vectorizer.fit_transform(processed_texts)
    print(f"–°–æ–∑–¥–∞–Ω–∞ TF-IDF –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–∞: {tfidf_matrix.shape}")
    
    # K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    print("–í—ã–ø–æ–ª–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(tfidf_matrix)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
    df_clean['cluster'] = clusters
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    silhouette_avg = silhouette_score(tfidf_matrix, clusters)
    print(f"Silhouette Score: {silhouette_avg:.3f}")
    
    return df_clean, clusters, tfidf_matrix, vectorizer

def analyze_clusters(df_clustered, vectorizer):
    """–ê–Ω–∞–ª–∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
    print(f"\n=== –ê–ù–ê–õ–ò–ó –ö–õ–ê–°–¢–ï–†–û–í ===")
    
    for cluster_id in sorted(df_clustered['cluster'].unique()):
        cluster_data = df_clustered[df_clustered['cluster'] == cluster_id]
        cluster_size = len(cluster_data)
        
        print(f"\n–ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ({cluster_size} –æ—Ç–∑—ã–≤–æ–≤):")
        
        # –¢–æ–ø –ø—Ä–æ–¥—É–∫—Ç—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        top_products = cluster_data['product_type'].value_counts().head(3)
        print(f"  –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã: {dict(top_products)}")
        
        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
        avg_length = cluster_data['review_text'].str.len().mean()
        print(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ü—Ä–∏–º–µ—Ä—ã –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        if len(cluster_data) > 0:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π DataFrame —Å –¥–ª–∏–Ω–æ–π —Ç–µ–∫—Å—Ç–∞
            cluster_with_length = cluster_data.copy()
            cluster_with_length['text_length'] = cluster_with_length['review_text'].str.len()
            examples = cluster_with_length.nsmallest(min(2, len(cluster_data)), 'text_length')
            
            print(f"  –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–∑—ã–≤–æ–≤:")
            for i, (_, row) in enumerate(examples.iterrows()):
                text = row['review_text'][:150] + "..." if len(row['review_text']) > 150 else row['review_text']
                print(f"    {i+1}. {text}")
        else:
            print(f"  –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–∑—ã–≤–æ–≤: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")

def visualize_clusters(df_clustered, tfidf_matrix):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
    print(f"\n=== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ===")
    
    # –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é PCA
    print("–°–æ–∑–¥–∞–µ–º 2D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é...")
    pca = PCA(n_components=2, random_state=42)
    coords_2d = pca.fit_transform(tfidf_matrix.toarray())
    
    # –ì—Ä–∞—Ñ–∏–∫
    plt.figure(figsize=(12, 8))
    scatter = plt.scatter(coords_2d[:, 0], coords_2d[:, 1], 
                         c=df_clustered['cluster'], cmap='tab10', alpha=0.6)
    plt.colorbar(scatter)
    plt.title('–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (PCA)', fontsize=16)
    plt.xlabel(f'–ü–µ—Ä–≤–∞—è –≥–ª–∞–≤–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ ({pca.explained_variance_ratio_[0]:.1%} –≤–∞—Ä–∏–∞—Ü–∏–∏)')
    plt.ylabel(f'–í—Ç–æ—Ä–∞—è –≥–ª–∞–≤–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ ({pca.explained_variance_ratio_[1]:.1%} –≤–∞—Ä–∏–∞—Ü–∏–∏)')
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã
    for cluster_id in df_clustered['cluster'].unique():
        cluster_center = coords_2d[df_clustered['cluster'] == cluster_id].mean(axis=0)
        plt.annotate(f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}', 
                    xy=cluster_center, 
                    xytext=(5, 5), 
                    textcoords='offset points',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8),
                    fontsize=10, fontweight='bold')
    
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    import os
    os.makedirs('/Users/mishantique/Desktop/Projects/gazprombank_hachaton/reports/clustering', exist_ok=True)
    output_path = '/Users/mishantique/Desktop/Projects/gazprombank_hachaton/reports/clustering/simple_clustering_demo.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    plt.show()

def create_summary_report(df_clustered, original_product_counts):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    print(f"\n=== –ö–†–ê–¢–ö–ò–ô –û–¢–ß–ï–¢ ===")
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    n_clusters = len(df_clustered['cluster'].unique())
    total_reviews = len(df_clustered)
    
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤: {total_reviews}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
    print(f"–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞: {total_reviews/n_clusters:.1f}")
    
    # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–∞–º
    print(f"\n–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Ç–∏–ø–∞–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤:")
    cluster_product_match = {}
    
    for cluster_id in sorted(df_clustered['cluster'].unique()):
        cluster_data = df_clustered[df_clustered['cluster'] == cluster_id]
        dominant_product = cluster_data['product_type'].value_counts().index[0]
        dominant_count = cluster_data['product_type'].value_counts().iloc[0]
        purity = dominant_count / len(cluster_data)
        
        cluster_product_match[cluster_id] = {
            'product': dominant_product,
            'purity': purity,
            'size': len(cluster_data)
        }
        
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {dominant_product} ({purity:.1%} —á–∏—Å—Ç–æ—Ç—ã, {len(cluster_data)} –æ—Ç–∑—ã–≤–æ–≤)")
    
    # –û–±—â–∞—è —á–∏—Å—Ç–æ—Ç–∞
    avg_purity = np.mean([info['purity'] for info in cluster_product_match.values()])
    print(f"\n–°—Ä–µ–¥–Ω—è—è —á–∏—Å—Ç–æ—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {avg_purity:.1%}")
    
    if avg_purity > 0.6:
        print("‚úÖ –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
    elif avg_purity > 0.4:
        print("‚ö†Ô∏è  –°—Ä–µ–¥–Ω—è–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
    else:
        print("‚ùå –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
    
    return cluster_product_match

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("üöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò –û–¢–ó–´–í–û–í –ë–ê–ù–ö–ê")
    print("=" * 60)
    
    # –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
    data_path = '/Users/mishantique/Desktop/Projects/gazprombank_hachaton/data/raw/sravni_ru/sravni_ru.json'
    
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = load_data(data_path, sample_size=1000)  # –ë–µ—Ä–µ–º –≤—ã–±–æ—Ä–∫—É –¥–ª—è –¥–µ–º–æ
        
        # 2. –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        product_counts = analyze_data(df)
        
        # 3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        df_clustered, clusters, tfidf_matrix, vectorizer = simple_tfidf_clustering(df, n_clusters=8)
        
        # 4. –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        analyze_clusters(df_clustered, vectorizer)
        
        # 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        visualize_clusters(df_clustered, tfidf_matrix)
        
        # 6. –û—Ç—á–µ—Ç
        cluster_info = create_summary_report(df_clustered, product_counts)
        
        print(f"\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ reports/")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
