#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –æ—Ç–∑—ã–≤–æ–≤ –ì–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫–∞ —Å Sravni.ru
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤—Ä—É—á–Ω—É—é, –∑–∞—Ç–µ–º –ø–∞—Ä—Å–µ—Ä —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –æ—Ç–∑—ã–≤—ã —Å –¥–∞—Ç–∞–º–∏
"""

import json
import time
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from urllib.parse import urlparse, parse_qs

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from webdriver_setup import setup_chrome_driver

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class InteractiveGazprombankParser:
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.driver = None
        self.data_dir = Path("data/sravni_ru")
        self.data_dir.mkdir(parents=True, exist_ok=True)

    def _setup_driver(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ–±-–¥—Ä–∞–π–≤–µ—Ä–∞"""
        self.driver = setup_chrome_driver(headless=self.headless)
        logger.info("–í–µ–±-–¥—Ä–∞–π–≤–µ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _extract_product_info(self, url: str) -> tuple[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–¥—É–∫—Ç–µ –∏–∑ URL"""
        # –°–ª–æ–≤–∞—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π URL-–ø—É—Ç–µ–π –∏ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç–æ–≤
        product_mapping = {
            'avtokredity': ('–ê–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç—ã', 'avtokredity'),
            'debetovye-karty': ('–î–µ–±–µ—Ç–æ–≤—ã–µ –∫–∞—Ä—Ç—ã', 'debetovye_karty'),
            'kreditnye-karty': ('–ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã', 'kreditnye_karty'), 
            'karty': ('–ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã', 'kreditnye_karty'),
            'kredity': ('–ö—Ä–µ–¥–∏—Ç—ã –Ω–∞–ª–∏—á–Ω—ã–º–∏', 'kredity'),
            'vklady': ('–í–∫–ª–∞–¥—ã', 'vklady'),
            'ipoteka': ('–ò–ø–æ—Ç–µ–∫–∞', 'ipoteka'),
            'refinansirovanie-kreditov': ('–†–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–æ–≤', 'refinansirovanie_kreditov'),
            'refinansirovanie-ipoteki': ('–†–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–ø–æ—Ç–µ–∫–∏', 'refinansirovanie_ipoteki'),
            'obsluzhivanie': ('–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ', 'obsluzhivanie'),
            'distancionnoe-obsluzhivanie': ('–î–∏—Å—Ç–∞–Ω—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ', 'distancionnoe_obsluzhivanie'),
            'mobile-app': ('–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', 'mobile_app'),
            'exchange': ('–û–±–º–µ–Ω –≤–∞–ª—é—Ç', 'exchange'),
            'other-service': ('–ü—Ä–æ—á–∏–µ —É—Å–ª—É–≥–∏', 'other_service'),
            'acquiring': ('Acquiring', 'acquiring'),
            'rko': ('RKO', 'rko'),
            'remittance': ('Remittance', 'remittance'),
            'conditions': ('Conditions', 'conditions')
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å –∏–∑ URL
        parsed = urlparse(url)
        path_parts = [part for part in parsed.path.split('/') if part]
        
        # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø—Ä–æ–¥—É–∫—Ç –≤ –ø—É—Ç–∏
        for part in path_parts:
            if part in product_mapping:
                return product_mapping[part]
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —á–∞—Å—Ç–∏ –ø—É—Ç–∏
        if len(path_parts) >= 2:
            potential_product = path_parts[-2]  # –ë–µ—Ä–µ–º —á–∞—Å—Ç—å –ø–µ—Ä–µ–¥ 'otzyvy'
            clean_product = potential_product.replace('-', '_')
            product_name = potential_product.replace('-', ' ').title()
            return (product_name, clean_product)
        
        # Fallback
        return ('–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç', 'unknown_product')

    def wait_for_manual_scroll(self, main_url: str) -> List[str]:
        """–û–∂–∏–¥–∞–µ—Ç —Ä—É—á–Ω–æ–π –ø—Ä–æ–∫—Ä—É—Ç–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º, –∑–∞—Ç–µ–º —Å–æ–±–∏—Ä–∞–µ—Ç URL –æ—Ç–∑—ã–≤–æ–≤"""
        self._setup_driver()
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {main_url}")
        self.driver.get(main_url)
        time.sleep(3)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ–¥—É–∫—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        product_name, _ = self._extract_product_info(main_url)
        
        print("\n" + "="*80)
        print(f"üéÆ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú –ü–ê–†–°–ò–ù–ì–ê: {product_name.upper()}")
        print("="*80)
        print("üìã –ò–ù–°–¢–†–£–ö–¶–ò–Ø:")
        print("   1. –ë—Ä–∞—É–∑–µ—Ä –æ—Ç–∫—Ä—ã—Ç –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        print("   2. –ü—Ä–æ–∫—Ä—É—Ç–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É –í–†–£–ß–ù–£–Æ –¥–æ —Å–∞–º–æ–≥–æ –∫–æ–Ω—Ü–∞")
        print("   3. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ –æ—Ç–∑—ã–≤—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        print("   4. –ö–æ–≥–¥–∞ –∑–∞–∫–æ–Ω—á–∏—Ç–µ –ø—Ä–æ–∫—Ä—É—Ç–∫—É, –ù–ï –¢–†–û–ì–ê–ô–¢–ï –±—Ä–∞—É–∑–µ—Ä 5 —Å–µ–∫—É–Ω–¥")
        print("   5. –ü–∞—Ä—Å–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—á–Ω–µ—Ç —Å–±–æ—Ä URL –æ—Ç–∑—ã–≤–æ–≤")
        print()
        print("‚ö†Ô∏è  –í–ê–ñ–ù–û:")
        print("   - –ù–ï –∑–∞–∫—Ä—ã–≤–∞–π—Ç–µ –±—Ä–∞—É–∑–µ—Ä")
        print("   - –ù–ï –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –Ω–∞ –¥—Ä—É–≥–∏–µ –≤–∫–ª–∞–¥–∫–∏")
        print("   - –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–π—Ç–µ –º–µ–¥–ª–µ–Ω–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏")
        print("="*80)
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏
        last_scroll_time = time.time()
        last_height = 0
        stable_count = 0
        max_stable_time = 5  # 5 —Å–µ–∫—É–Ω–¥ –±–µ–∑–¥–µ–π—Å—Ç–≤–∏—è
        
        print("üîÑ –û–∂–∏–¥–∞–µ–º —Ä—É—á–Ω—É—é –ø—Ä–æ–∫—Ä—É—Ç–∫—É...")
        print("üí° –°–æ–≤–µ—Ç: –ø—Ä–æ–∫—Ä—É—á–∏–≤–∞–π—Ç–µ –º–µ–¥–ª–µ–Ω–Ω–æ, –¥–∞–≤–∞—è –≤—Ä–µ–º—è –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤")
        
        while True:
            current_time = time.time()
            current_height = self.driver.execute_script("return window.pageYOffset")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –ª–∏ –ø–æ–∑–∏—Ü–∏—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∏
            if current_height != last_height:
                last_scroll_time = current_time
                last_height = current_height
                stable_count = 0
                print(f"üìç –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –ø–æ–∑–∏—Ü–∏—è: {current_height}")
            else:
                # –°—á–∏—Ç–∞–µ–º –≤—Ä–µ–º—è –±–µ–∑–¥–µ–π—Å—Ç–≤–∏—è
                idle_time = current_time - last_scroll_time
                if idle_time >= max_stable_time:
                    print(f"\n‚úÖ –ë–µ–∑–¥–µ–π—Å—Ç–≤–∏–µ {idle_time:.1f} —Å–µ–∫ - –Ω–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä URL!")
                    break
                else:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π –æ—Ç—Å—á–µ—Ç
                    remaining = max_stable_time - idle_time
                    print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∏: {remaining:.1f} —Å–µ–∫", end="\r")
            
            time.sleep(0.5)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 0.5 —Å–µ–∫—É–Ω–¥
        
        print("\nüîç –ù–ê–ß–ò–ù–ê–ï–ú –°–ë–û–† URL –û–¢–ó–´–í–û–í...")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ URL –æ—Ç–∑—ã–≤–æ–≤
        review_links = []
        unique_review_ids = set()
        
        selectors = [
            "a[href*='/otzyvy/'][href*='/']",
            "a[class*='review'][href*='/']", 
            "a[href*='/bank/gazprombank/otzyvy/']"
        ]
        
        for selector in selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {selector}")
                
                for element in elements:
                    href = element.get_attribute('href')
                    if href and '/otzyvy/' in href:
                        # –û—á–∏—â–∞–µ–º URL –æ—Ç —è–∫–æ—Ä–µ–π
                        clean_url = href.split('#')[0]
                        if clean_url.endswith('/'):
                            clean_url = clean_url[:-1]
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –æ—Ç–∑—ã–≤ —Å —á–∏—Å–ª–æ–≤—ã–º ID
                        url_parts = clean_url.split('/')
                        if len(url_parts) >= 2 and url_parts[-1].isdigit():
                            review_id = url_parts[-1]
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
                            if review_id not in unique_review_ids:
                                unique_review_ids.add(review_id)
                                review_links.append(clean_url + '/')
                                
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º {selector}: {e}")
                continue
        
        logger.info(f"üéâ –°–æ–±—Ä–∞–Ω–æ {len(review_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL –æ—Ç–∑—ã–≤–æ–≤")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–ë–û–†–ê URL:")
        print(f"   üîó –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(review_links)}")
        
        return review_links

    def _parse_date(self, date_text: str) -> Optional[str]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
        if not date_text:
            return None
            
        try:
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
            date_text = date_text.strip()
            
            # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∑–∞–º–µ–Ω—ã –º–µ—Å—è—Ü–µ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
            months_ru = {
                '—è–Ω–≤–∞—Ä—è': '01', '—Ñ–µ–≤—Ä–∞–ª—è': '02', '–º–∞—Ä—Ç–∞': '03', '–∞–ø—Ä–µ–ª—è': '04',
                '–º–∞—è': '05', '–∏—é–Ω—è': '06', '–∏—é–ª—è': '07', '–∞–≤–≥—É—Å—Ç–∞': '08',
                '—Å–µ–Ω—Ç—è–±—Ä—è': '09', '–æ–∫—Ç—è–±—Ä—è': '10', '–Ω–æ—è–±—Ä—è': '11', '–¥–µ–∫–∞–±—Ä—è': '12'
            }
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –¥–∞—Ç—ã –≤–∏–¥–∞ "23 –∞–≤–≥—É—Å—Ç–∞ 2024"
            pattern = r'(\d{1,2})\s+([–∞-—è]+)\s+(\d{4})'
            match = re.search(pattern, date_text.lower())
            
            if match:
                day, month_name, year = match.groups()
                if month_name in months_ru:
                    month = months_ru[month_name]
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ ISO —Ñ–æ—Ä–º–∞—Ç
                    formatted_date = f"{year}-{month}-{day.zfill(2)}"
                    return formatted_date
            
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            return date_text
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã '{date_text}': {e}")
            return date_text

    def parse_single_review(self, review_url: str, product_name: str) -> Optional[Dict[str, str]]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω –æ—Ç–∑—ã–≤ –ø–æ URL —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –¥–∞—Ç—ã"""
        try:
            logger.info(f"–ü–∞—Ä—Å–∏–º –æ—Ç–∑—ã–≤: {review_url}")
            self.driver.get(review_url)
            time.sleep(2)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º—ã –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            current_url = self.driver.current_url
            if review_url not in current_url and not current_url.endswith(review_url.split('/')[-2] + '/'):
                logger.warning(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π URL: –æ–∂–∏–¥–∞–ª—Å—è {review_url}, –ø–æ–ª—É—á–µ–Ω {current_url}")
                return None
            
            # –ò—â–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
            text_selectors = [
                "div[class*='review-card_text__']",
                "div[class*='review-text']", 
                "div[class*='content']",
                "article",
                "main",
                "section"
            ]
            
            # –ò—â–µ–º –¥–∞—Ç—É –æ—Ç–∑—ã–≤–∞
            date_selectors = [
                "div[class*='h-color-D30__1aja02n__1w661f']",  # –û—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä
                "div[class*='h-color-D30']",
                "div[class*='1aja02n']",
                "div[class*='1w661f']",
                "span[class*='date']",
                "time",
                "[datetime]"
            ]
            
            full_text = None
            review_date = None
            
            # –ü–∞—Ä—Å–∏–º —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
            for selector in text_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
                        elem = elements[0]
                        text = elem.text.strip()
                        
                        if len(text) > 100:
                            if not any(pattern in text.lower() for pattern in [
                                '–Ω–∞–≤–∏–≥–∞—Ü–∏—è', '–º–µ–Ω—é', '–≤–æ–π—Ç–∏', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è',
                                '–ø–æ–¥–±–æ—Ä –∫—Ä–µ–¥–∏—Ç–∞', '—Å—Ä–∞–≤–Ω–∏ –≤ –º–æ–±–∏–ª—å–Ω–æ–º', '–∫ —Å–ø–∏—Å–∫—É –æ—Ç–∑—ã–≤–æ–≤',
                                '–¥—Ä—É–≥–∏–µ –æ—Ç–∑—ã–≤—ã', '–æ—Å—Ç–∞–≤—å—Ç–µ –æ—Ç–∑—ã–≤', '—Ä–µ–π—Ç–∏–Ω–≥ –±–∞–Ω–∫–æ–≤', '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π'
                            ]):
                                full_text = text
                                logger.info(f"–ù–∞–π–¥–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–∑—ã–≤ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º {selector}: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                                break
                except Exception:
                    continue
            
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –æ—Ç–∑—ã–≤–∞
            for selector in date_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        for elem in elements:
                            date_text = elem.text.strip()
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –¥–∞—Ç—É
                            if date_text and (
                                re.search(r'\d{1,2}\s+[–∞-—è]+\s+\d{4}', date_text.lower()) or  # "23 –∞–≤–≥—É—Å—Ç–∞ 2024"
                                re.search(r'\d{1,2}\.\d{1,2}\.\d{4}', date_text) or  # "23.08.2024"
                                any(month in date_text.lower() for month in [
                                    '—è–Ω–≤–∞—Ä—è', '—Ñ–µ–≤—Ä–∞–ª—è', '–º–∞—Ä—Ç–∞', '–∞–ø—Ä–µ–ª—è', '–º–∞—è', '–∏—é–Ω—è',
                                    '–∏—é–ª—è', '–∞–≤–≥—É—Å—Ç–∞', '—Å–µ–Ω—Ç—è–±—Ä—è', '–æ–∫—Ç—è–±—Ä—è', '–Ω–æ—è–±—Ä—è', '–¥–µ–∫–∞–±—Ä—è'
                                ])
                            ):
                                review_date = self._parse_date(date_text)
                                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞ –æ—Ç–∑—ã–≤–∞ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º {selector}: {date_text} -> {review_date}")
                                break
                    if review_date:
                        break
                except Exception:
                    continue
            
            if full_text:
                # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                cleaned_text = ' '.join(full_text.split())
                
                if len(cleaned_text) > 50:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –æ—Ç–∑—ã–≤–∞ –∏–∑ URL
                    review_id = review_url.rstrip('/').split('/')[-1]
                    
                    review_data = {
                        "review_id": review_id,
                        "review_text": cleaned_text,
                        "review_date": review_date,  # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞—Ç—É
                        "url": review_url,
                        "parsed_at": datetime.now().isoformat(),
                        "bank_name": "gazprombank",
                        "product_type": product_name
                    }
                    
                    logger.info(f"–£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω –æ—Ç–∑—ã–≤ –¥–ª–∏–Ω–æ–π {len(cleaned_text)} —Å–∏–º–≤–æ–ª–æ–≤ —Å –¥–∞—Ç–æ–π {review_date} —Å URL {review_url}")
                    return review_data
                else:
                    logger.warning(f"–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({len(cleaned_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ —Å URL: {review_url}")
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ—Ç–∑—ã–≤–∞ {review_url}: {e}")
            return None

    def parse_all_reviews(self, main_url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ –æ—Ç–∑—ã–≤—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–¥—É–∫—Ç–µ
        product_name, filename_base = self._extract_product_info(main_url)
        
        # –≠—Ç–∞–ø 1: –†—É—á–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ –∏ —Å–±–æ—Ä URL
        print("üîÑ –≠–¢–ê–ü 1: –°–ë–û–† URL –û–¢–ó–´–í–û–í")
        review_urls = self.wait_for_manual_scroll(main_url)
        
        if not review_urls:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ URL –æ—Ç–∑—ã–≤–æ–≤")
            return []
        
        # –≠—Ç–∞–ø 2: –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–≥–æ –æ—Ç–∑—ã–≤–∞
        print(f"\nüîÑ –≠–¢–ê–ü 2: –ü–ê–†–°–ò–ù–ì {len(review_urls)} –û–¢–ó–´–í–û–í")
        all_reviews = []
        
        for i, review_url in enumerate(review_urls, 1):
            print(f"üìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–∑—ã–≤ {i}/{len(review_urls)}")
            
            review_data = self.parse_single_review(review_url, product_name)
            if review_data:
                review_data["review_id"] = str(i)  # –ü–µ—Ä–µ–Ω—É–º–µ—Ä–æ–≤—ã–≤–∞–µ–º
                all_reviews.append(review_data)
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(1)
        
        return all_reviews, filename_base

    def save_reviews(self, reviews: List[Dict], filename: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–∑—ã–≤—ã –≤ JSON —Ñ–∞–π–ª"""
        if not reviews:
            logger.warning("–ù–µ—Ç –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        json_path = self.data_dir / f"{filename}.json"
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(reviews, f, ensure_ascii=False, indent=2)
            logger.info(f"–û—Ç–∑—ã–≤—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON: {json_path}")
            return json_path
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ JSON: {e}")
            return None

    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç –≤–µ–±-–¥—Ä–∞–π–≤–µ—Ä"""
        if self.driver:
            self.driver.quit()
            logger.info("–í–µ–±-–¥—Ä–∞–π–≤–µ—Ä –∑–∞–∫—Ä—ã—Ç")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üöÄ –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ü–ê–†–°–ï–† –ì–ê–ó–ü–†–û–ú–ë–ê–ù–ö–ê")
    print("="*60)
    print()
    
    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º URL —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    url = input("üìù –í–≤–µ–¥–∏—Ç–µ URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç–∑—ã–≤–æ–≤: ").strip()
    
    if not url:
        print("‚ùå URL –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º!")
        return
    
    if 'gazprombank' not in url or 'otzyvy' not in url:
        print("‚ùå URL –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—Ç–∑—ã–≤—ã –ì–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫–∞ —Å —Å–∞–π—Ç–∞ Sravni.ru")
        return
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä "–í—Å–µ" –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if 'filterby=all' not in url:
        separator = '&' if '?' in url else '?'
        url = f"{url}{separator}filterby=all"
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Ñ–∏–ª—å—Ç—Ä '–í—Å–µ': {url}")
    
    parser = InteractiveGazprombankParser(headless=False)  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤–∏–¥–∏–º—ã–π –±—Ä–∞—É–∑–µ—Ä
    
    try:
        print("\n‚úÖ –†—É—á–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ + –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä + –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç")
        print()
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤
        reviews, filename = parser.parse_all_reviews(url)
        
        if reviews:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            saved_path = parser.save_reviews(reviews, filename)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            print(f"\nüéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
            print(f"üìä –°–æ–±—Ä–∞–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤")
            
            if reviews:
                lengths = [len(r['review_text']) for r in reviews]
                avg_length = sum(lengths) / len(lengths)
                
                # –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç
                dates_found = len([r for r in reviews if r.get('review_date')])
                dates_percentage = (dates_found / len(reviews)) * 100
                
                print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                print(f"   üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–∑—ã–≤–∞: {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"   üìö –°–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–∑—ã–≤: {max(lengths)} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"   üìÑ –°–∞–º—ã–π –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–∑—ã–≤: {min(lengths)} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"   üìÖ –ù–∞–π–¥–µ–Ω–æ –¥–∞—Ç: {dates_found}/{len(reviews)} ({dates_percentage:.1f}%)")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∞—Ç
                sample_dates = [r.get('review_date') for r in reviews[:5] if r.get('review_date')]
                if sample_dates:
                    print(f"   üìÖ –ü—Ä–∏–º–µ—Ä—ã –¥–∞—Ç: {', '.join(sample_dates[:3])}")
            
            if saved_path:
                print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {saved_path}")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –æ—Ç–∑—ã–≤—ã")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
    finally:
        parser.close()

if __name__ == "__main__":
    main()
